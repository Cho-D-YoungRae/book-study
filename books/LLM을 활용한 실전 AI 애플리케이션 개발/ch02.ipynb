{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.1 토큰화 코드"
      ],
      "metadata": {
        "id": "lb69MbCo0OQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 띄어쓰기 단위로 분리\n",
        "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
        "input_text_list = input_text.split()\n",
        "print(\"input_text_list: \", input_text_list)\n",
        "\n",
        "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
        "str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n",
        "idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n",
        "print(\"str2idx: \", str2idx)\n",
        "print(\"idx2str: \", idx2str)\n",
        "\n",
        "# 토큰을 토큰 아이디로 변환\n",
        "input_ids = [str2idx[word] for word in input_text_list]\n",
        "print(\"input_ids: \", input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj-0PjJ98v8q",
        "outputId": "5e38336c-0e9b-4c51-e2b9-22346d79954a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
            "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
            "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
            "input_ids:  [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.2 토큰 아이디에서 벡터로 변환"
      ],
      "metadata": {
        "id": "51EXsKZm9TvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 16\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "\n",
        "input_embeddings = embed_layer(torch.tensor(input_ids))\n",
        "input_embeddings = input_embeddings.unsqueeze(0)\n",
        "print(f\"shape: {input_embeddings.shape}\")\n",
        "print(f\"embedding: {input_embeddings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZKIb0xk9uDo",
        "outputId": "865e5ac9-b07f-443a-864f-5a3b76691e03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([1, 5, 16])\n",
            "embedding: tensor([[[-0.8989, -0.7829,  1.0097,  0.6984,  0.5762, -1.1438, -0.6242,\n",
            "           0.7963,  0.5647, -0.2268, -0.4924, -0.7774, -0.1097, -0.2298,\n",
            "           2.1904,  0.2229],\n",
            "         [ 0.0757,  1.5489, -0.6447,  0.4154,  0.6332, -0.1329, -1.2106,\n",
            "          -1.3371, -0.1560, -0.2890,  1.3018, -0.2338,  1.7527,  0.0892,\n",
            "           1.4164,  0.6387],\n",
            "         [-1.1524, -0.3284, -0.0551,  1.0712,  1.3251,  0.2536,  0.5346,\n",
            "           0.4892, -0.3636, -1.9304, -1.1637,  0.1573, -0.8734,  1.4249,\n",
            "          -0.3997, -0.5594],\n",
            "         [-0.9178, -1.3168, -0.4967,  1.3440,  1.6285, -2.4251,  0.1618,\n",
            "           0.2290,  0.9579,  0.8979,  1.9504,  0.1880, -1.5089,  1.6096,\n",
            "           0.1364,  1.1089],\n",
            "         [-1.1592, -1.6617, -0.7340,  0.1359,  0.2002, -1.6263, -0.1228,\n",
            "          -0.8329, -1.5891,  0.3412, -0.1175,  0.2106,  0.6662, -0.4544,\n",
            "           0.4688,  2.2253]]], grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.3 절대적 위치 인코딩"
      ],
      "metadata": {
        "id": "2luSdRK4-JPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "max_position = 12\n",
        "# 토큰 임베딩 층 생성\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "# 위치 인코딩 층 생성\n",
        "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
        "\n",
        "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
        "print(f\"position_ids: {position_ids}\")\n",
        "position_encodings = position_embed_layer(position_ids)\n",
        "print(f\"position_encodings: {position_encodings}\")\n",
        "token_embeddings = embed_layer(torch.tensor(input_ids))\n",
        "token_embeddings = token_embeddings.unsqueeze(0)\n",
        "\n",
        "# 토큰 임베딩과 위치 인코딩을 더해 최종 입력 임베딩 생성\n",
        "input_embeddings = token_embeddings + position_encodings\n",
        "print(f\"input_embeddings.shape: {input_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxkRCbY9U1GL",
        "outputId": "b22faa5f-a089-4e1f-e7b7-a19cf5c555e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "position_ids: tensor([[0, 1, 2, 3, 4]])\n",
            "position_encodings: tensor([[[ 0.1850, -1.4910, -1.1346, -0.7657,  1.6132, -1.7511,  2.0782,\n",
            "           0.0796, -1.4046,  0.4035, -2.0391, -0.1013,  0.2305, -0.6535,\n",
            "           0.1593,  0.2376],\n",
            "         [ 0.5238,  0.0051, -0.7478,  0.8095, -0.3439, -2.1219, -0.3824,\n",
            "           0.1655,  0.0461,  0.7539, -1.1021, -0.3834, -1.1017, -0.9535,\n",
            "          -0.3936,  1.3097],\n",
            "         [ 0.5729, -0.8689, -0.8217,  0.6492,  2.6804, -0.6246, -0.4054,\n",
            "          -0.1487,  1.4473, -0.3233, -0.2010, -0.3219, -0.0690, -1.4358,\n",
            "           1.9119,  0.4605],\n",
            "         [ 0.6670, -0.4039, -0.6003,  0.2912, -0.3128,  1.5148, -0.8338,\n",
            "          -0.0050,  2.2244,  1.3744,  0.2087, -1.3347,  1.2319, -0.8684,\n",
            "           1.4215,  0.6391],\n",
            "         [-0.2436,  0.7457, -1.3710,  0.4665, -0.9259, -0.3234, -0.0589,\n",
            "          -0.4910, -0.0777,  1.1461,  0.9975, -1.1443, -0.7954,  0.3772,\n",
            "           0.8681, -1.4612]]], grad_fn=<EmbeddingBackward0>)\n",
            "input_embeddings.shape: torch.Size([1, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.4 쿼리, 키, 값 벡터를 만드는 nn.Linear 층"
      ],
      "metadata": {
        "id": "hP0jhDgbVrdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_dim = 16\n",
        "\n",
        "# 쿼리, 키, 값을 계산하기 위한 변환\n",
        "weight_q = nn.Linear(embedding_dim, head_dim)\n",
        "weight_k = nn.Linear(embedding_dim, head_dim)\n",
        "weight_v = nn.Linear(embedding_dim, head_dim)\n",
        "\n",
        "# 변환 수행\n",
        "querys = weight_q(input_embeddings) # (1, 5, 6)\n",
        "keys = weight_k(input_embeddings) # (1, 5, 6)\n",
        "values = weight_v(input_embeddings) # (1, 5, 6)"
      ],
      "metadata": {
        "id": "U6fqo70kYJ7F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.5. 스케일 점곱 방식의 어텐션"
      ],
      "metadata": {
        "id": "DY1qLZNKY1CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_attention(querys, keys, values, is_causal=False):\n",
        "  dim_k = querys.size(-1) # 16\n",
        "  scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim=-1)\n",
        "  return weights @ values\n"
      ],
      "metadata": {
        "id": "wCw7AFU6ZfGz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.6. 어텐션 연산의 입력과 출력"
      ],
      "metadata": {
        "id": "MMXxrq31Z12n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"원본 입력 형태: {input_embeddings.shape}\")\n",
        "\n",
        "after_attention_embeddings = compute_attention(querys, keys, values)\n",
        "\n",
        "print(f\"어텐션 적용 후 형태: {after_attention_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33JS1hlFbbBJ",
        "outputId": "f646ec95-95e7-45dc-95f8-4b0931058af5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 입력 형태: torch.Size([1, 5, 16])\n",
            "어텐션 적용 후 형태: torch.Size([1, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
        "    super().__init__()\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embed_dim, head_dim) # 쿼리 벡터 생성을 위한 선형 층\n",
        "    self.weight_k = nn.Linear(token_embed_dim, head_dim) # 키 벡터 생성을 위한 선형 층\n",
        "    self.weight_v = nn.Linear(token_embed_dim, head_dim) # 값 벡터 생성을 위한 선형 층\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    outputs = compute_attention(\n",
        "        self.weight_q(querys),  # 쿼리 벡터\n",
        "        self.weight_k(keys),    # 키 벡터\n",
        "        self.weight_v(values),  # 값 벡터\n",
        "        is_causal=self.is_causal\n",
        "    )\n",
        "    return outputs\n",
        "\n",
        "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
        "after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)"
      ],
      "metadata": {
        "id": "4TB5tVhMbcNw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.8. 멀티 헤드 어텐션 구현"
      ],
      "metadata": {
        "id": "99-UWoymvbfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
        "    super().__init__()\n",
        "    self.n_head = n_head\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
        "    self.concat_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    B, T, C = querys.size()\n",
        "    querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "    attention = compute_attention(querys, keys, values, is_causal=self.is_causal)\n",
        "    output = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
        "    output = self.concat_linear(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "n_head = 4\n",
        "mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
        "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
        "print(f\"shape: {after_attention_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7RxduNmwZub",
        "outputId": "f183af95-ac3c-43a4-e65b-711cff9c4f29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([1, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.9. 층 정규화 코드"
      ],
      "metadata": {
        "id": "6l7KOOGUxN0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm = nn.LayerNorm(embedding_dim)\n",
        "norm_x = norm(input_embeddings)\n",
        "print(f\"norm_x.shape: {norm_x.shape}\")\n",
        "\n",
        "print(f\"norm_x.mean(dim=-1).data: {norm_x.mean(dim=-1).data}\")\n",
        "print(f\"norm_x.std(dim=-1).data: {norm_x.std(dim=-1).data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OtvtVjPm_Fa",
        "outputId": "41d81051-6b31-45d4-81b1-7cd9098314e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm_x.shape: torch.Size([1, 5, 16])\n",
            "norm_x.mean(dim=-1).data: tensor([[0.0000e+00, 7.4506e-09, 2.2352e-08, 4.0978e-08, 1.4901e-08]])\n",
            "norm_x.std(dim=-1).data: tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.10. 피드 포워드 층 코드"
      ],
      "metadata": {
        "id": "Dm37PuQ9nTLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreLayerNormFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, dim_feedforward, dropout):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1\n",
        "    self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2\n",
        "    self.dropout1 = nn.Dropout(dropout) # 드랍아웃 층 1\n",
        "    self.dropout2 = nn.Dropout(dropout) # 드랍아웃 층 2\n",
        "    self.activation = nn.GELU() # 활성 함수\n",
        "    self.norm = nn.LayerNorm(d_model) # 층 정규화\n",
        "\n",
        "  def forward(self, src):\n",
        "    x = self.norm(src)\n",
        "    x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
        "    x = self.dropout2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oyk3LULvnxl9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.11. 인코더 층\n"
      ],
      "metadata": {
        "id": "z4fn9bqDoOve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "    super().__init__()\n",
        "    self.attn = MultiheadAttention(d_model, d_model, nhead) # 멀티 헤드 어텐션 클래스\n",
        "    self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n",
        "    self.dropout1 = nn.Dropout(dropout) # 드롭아웃\n",
        "    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) # 피드 포워드\n",
        "\n",
        "  def forward(self, src):\n",
        "    norm_x = self.norm1(src)\n",
        "    attn_output = self.attn(norm_x, norm_x, norm_x)\n",
        "    x = src + self.droppout1(attn_output) # 잔차 연결\n",
        "\n",
        "    # 피드 포워드\n",
        "    x = self.feed_forward(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "tgsNRoVKpDVE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.12. 인코더 구현"
      ],
      "metadata": {
        "id": "AzKyUFbRpsgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, encoder_layer, num_layers):\n",
        "    super().__init__()\n",
        "    self.layers = get_clones(encoder_layer, num_layers)\n",
        "    self.num_layers = num_layers\n",
        "    self.norm = norm\n",
        "\n",
        "  def forward(self, src):\n",
        "    output = src\n",
        "    for mod in self.layers:\n",
        "      output = mod(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "IreSe4OgrbbE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.13. 디코더에서 어텐션 연산(마스크 어텐션)"
      ],
      "metadata": {
        "id": "_SDVBU6sr-m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_attention(querys, keys, values, is_causal=False):\n",
        "  def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "    dim_k = querys.size(-1) # 16\n",
        "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k) # (1, 5, 5)\n",
        "    if is_causal:\n",
        "      query_length = querys.size(2)\n",
        "      key_length = keys.size(2)\n",
        "      temp_mask = torch.ones(query_length, key_length, dtype=torch.bool).tril(diagonal=0)\n",
        "      scores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n",
        "    weights = F.softmax(scores, dim=-1) # (1, 5, 5)\n",
        "    return weights @ values # (1, 5, 16)"
      ],
      "metadata": {
        "id": "qOpDTlDutKNy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.14. 크로스 어텐션이 포함된 디코더 층"
      ],
      "metadata": {
        "id": "QUZJYAZMt6Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attn = MultiheadAttention(d_model, d_model, nhead)\n",
        "    self.multihead_attn = MultiheadAttention(d_model, d_model, nhead)\n",
        "    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, tgt, encoder_output, is_causal=True):\n",
        "    # 셀프 어텐션 연산\n",
        "    x = self.norm1(tgt)\n",
        "    x = x + self.dropout1(self.self_attn(x, x, x, is_causal=is_causal))\n",
        "    # 크로스 어텐션 연산\n",
        "    x = self.norm2(x)\n",
        "    x = x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output))\n",
        "    # 피드 포워드 연산\n",
        "    x = self.feed_forward(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-iAJDNMbvc-2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2.15. 디코더 구현"
      ],
      "metadata": {
        "id": "JPZnlvJewOdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, decoder_layer, num_layers):\n",
        "    super().__init__()\n",
        "    self.layers = get_clones(decoder_layer, num_layers)\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def forward(self, tgt, src):\n",
        "    output = tgt\n",
        "    for mod in self.layers:\n",
        "        output = mod(output, src)\n",
        "    return output"
      ],
      "metadata": {
        "id": "MRQzJGfMwRIo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgXmhN8EwTeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}