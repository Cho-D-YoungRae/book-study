{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 09. 오차 역전파에서 딥러닝으로"
      ],
      "metadata": {
        "id": "15dEW9qA0f-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 기울기 소실 문젱와 활성화 함수\n",
        "\n",
        "깊은 층을 만들어 보니 출력층에서 시작된 가중치 업데이트가 처음 층까지 전달되지 않는 현상이 생기는 문제\n",
        "\n",
        "활성화 함수로 사용된 시그모이드 함수의 특성 때문\n",
        "\n",
        "시그모이드 함수를 미분하면 최대치는 0.25 이므로 계속 곱하다 보면 0에 가까워짐\n",
        "\n",
        "따라서 여러 층을 거칠 수록 기울기가 사라져 가중치를 수정하기 어려워짐\n",
        "\n",
        "이를 해결하고자 렐루(ReLU) 라는 새로운 활성화 함수 제안\n",
        "\n",
        "x가 0보다 크기만 하면 미분 값은 1이 됨\n",
        "\n",
        "하이퍼볼릭 탄젠트(hyperbolic tangent) 함수나 소프트 플러스(softplus) 함수 등 좀 더 나은 활성화 함수를 만들기 위한 노력이 이어지고 있음"
      ],
      "metadata": {
        "id": "Yk0XZ4iCfJA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 속도와 정확도 문제를 해결하는 고급 경사 하강법\n",
        "\n",
        "경사 하강법은 정확하게 가중치를 찾아가지만, 계산량이 매우 많다는 단점\n",
        "\n",
        "### 확률적 경사 하강법\n",
        "\n",
        "경사 하강법은 한 번 업데이트할 때마다 전체 데이터를 미분하므로 속도가 느릴 뿐 아니라, 최적 해를 찾기 전에 최적화 과정이 멈출 수도 있음\n",
        "\n",
        "> 확률적 경사 하강법(Stochastic Gradient Descent, SGD) 는 경사 하강법의 이러한 단점을 보완한 방법\n",
        "\n",
        "전체 데이터를 사용하는 것이 아니라 랜덤하게 추출한 일부 데이터만 사용하기 때문에 빠르고 더 자주 업데이트 할 수 있다는 장점\n",
        "\n",
        "랜덤한 일부 데이터를 사용하는 만큼 확률적 경사 하강법은 중간 결과의 진폭이 크고 불안정해 보일 수 있음\n",
        "\n",
        "하지만 속도가 확연히 빠르면서도 최적 해에 근사한 값을 찾아낸다는 장점 덕분에 경사 하강법의 대안으로 사용되고 있음\n",
        "\n",
        "### 모멘텀\n",
        "\n",
        "모멘텀(momentum)이란 단어는 '관성, 탄력, 가속도'라는 뜻\n",
        "\n",
        "모멘텀 확률적 경사 하강법(모멘텀 SGD)란 말 그대로 경사 하강법에 탄력을 더해 주는 것\n",
        "\n",
        "경사 하강법과 마찬가지로 매번 기울기를 구하지만, 이를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향(+, -)를 참고해 같은 방향으로 일정한 미율만 수정되게 하는 방법\n",
        "\n",
        "따라서 수정 방향이 양수(+) 방향으로 한번, 음수(-) 방향으로 한 번 지그재그로 일어나는 현상이 줄어들고, 이전 이동 값을 고려해 일정 비율만큼 다음 값을 결정하므로 관성 효과를 낼 수 있음"
      ],
      "metadata": {
        "id": "2ZxjD9KOfirm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금은 정확도와 속도를 모두 향상시킨 아담(adam)이라는 고급 경사 하강법이 가장 많이 쓰이고 있음"
      ],
      "metadata": {
        "id": "dghLgNwonUA5"
      }
    }
  ]
}