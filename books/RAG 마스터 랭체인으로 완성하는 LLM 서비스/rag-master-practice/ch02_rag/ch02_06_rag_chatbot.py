from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
import streamlit as st
from dotenv import load_dotenv

load_dotenv()

@st.cache_resource
def process_pdf() -> list[Document]:
    import requests
    import os
    from tempfile import NamedTemporaryFile

    url = "https://raw.githubusercontent.com/langchain-kr/langchain-tutorial/refs/heads/main/Ch02.%20RAG/Data/2024_KB_%EB%B6%80%EB%8F%99%EC%82%B0_%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%B5%9C%EC%A2%85.pdf"

    resp = requests.get(url, stream=True)
    resp.raise_for_status()

    with NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
        tmp.write(resp.content)
        tmp_path = tmp.name

    try:
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()
        print(f'ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}')
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        return text_splitter.split_documents(documents)
    finally:
        os.remove(tmp_path)

@st.cache_resource
def initialize_vectorstore() -> Chroma:
    chunks = process_pdf()
    embeddings = OpenAIEmbeddings()

    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings
    )

    print(f'ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}')
    return vectorstore


@st.cache_resource
def initialize_prompt() -> ChatPromptTemplate:
    template = """ë‹¹ì‹ ì€ KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

    ì»¨í…ìŠ¤íŠ¸: {context}
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            ("placeholder", "{chat_history}"),
            ("human", "{question}")
        ]
    )

    print("í”„ë¡¬í”„íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆì‹œ:")
    print(prompt.format(context="ì»¨í…ìŠ¤íŠ¸ ì˜ˆì‹œ", chat_history=["ëŒ€í™” ê¸°ë¡ ì˜ˆì‹œ1", "ëŒ€í™” ê¸°ë¡ ì˜ˆì‹œ2"], question="ì§ˆë¬¸ ì˜ˆì‹œ"))

    return prompt


def format_docs(docs: list[Document]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)


def initialize_chain() -> RunnableWithMessageHistory:
    retriever = initialize_vectorstore().as_retriever(search_kwargs={"k": 3})
    prompt = initialize_prompt()

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    base_chain = (
            RunnablePassthrough.assign(
                context=lambda x: format_docs(retriever.invoke(x['question']))
            )
            | prompt
            | model
            | StrOutputParser()
    )
    chat_history = ChatMessageHistory()
    chain_with_memory = RunnableWithMessageHistory(
        base_chain,
        lambda session_id: chat_history,  # ì„¸ì…˜ IDë³„ ëŒ€í™” ê¸°ë¡ ìƒì„±
        input_messages_key="question",
        history_messages_key="chat_history",
    )
    return chain_with_memory

def main():
    st.set_page_config(page_title="KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì±—ë´‡", page_icon="ğŸ ")
    st.title("ğŸ  KB ë¶€ë™ì‚° ë³´ê³ ì„œ AI ì–´ë“œë°”ì´ì €")
    st.caption("2024 KB ë¶€ë™ì‚° ë³´ê³ ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ì²´ì¸ ì´ˆê¸°í™”
        chain = initialize_chain()

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                response = chain.invoke(
                    {"question": prompt},
                    {"configurable": {"session_id": "streamlit_session"}}
                )
                st.markdown(response)

        st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()
