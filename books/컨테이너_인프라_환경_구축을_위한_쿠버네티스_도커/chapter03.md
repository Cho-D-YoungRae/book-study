# Chapter 03. 컨테이너를 다루는 표준 아키텍처, 쿠버네티스

- `컨테이너 인프라 환경`: 리눅스 운영 체제의 커널 하나에서 여러 개의 컨테이너가 격리된 상태로 실행되는 인프라 환경
- `컨테이너`: 하나 이상의 목적을 위해 독립적으로 작동하는 프로세스
- 기업 환경에서는 다수의 관리자가 수백 또는 수천 대의 서버를 함께 관리하기 때문에 일관성을 유지하는 것이 매우 중요
- 가상화 환경에서는 각각의 가상 머신이 모두 독립적인 운영 체제 커널을 가지고 있어야 하기 때문에 그만큼 자원을 더 소모해야 하고 성능이 떨어짐
- 컨테이너 인프라 환경은 운영 체제 커널 하나에 컨테이너 여러 개가 격리된 형태로 실행되기 때문에 자원을 효율적으로 사용할 수 있고 거치는 단계가 적어서 속도도 훨씬 빠름

## 3.1 쿠버네티스 이해하기

- `쿠버네티스`: 컨테이너 오케스트레이션
  - `오케스트레이션(Orchestration)`: 복잡한 단계를 관리하고 요소들의 유기적인 관계를 미리 정의해 손쉽게 사용하도록 서비스를 제공하는 것

### 3.1.3 쿠버네티스 구성하기

#### config.sh (쿠버네티스 설치 사전 조건 스크립트 파일)

```sh
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
```

- 브리지 네트워크를 통과하는 IPv4와 IPv6의 패킷을 iptables가 관리하게 설정
- 파드의 통신을 iptables로 제어
- 필요에 따라 IPVS(IP Virtual Server) 같은 방식으로도 구성 가능

```sh
modprobe br_netfilter
```

- br_netfilter 커널 모듈을 사용해 브리지로 네트워크를 구성
- 이때 IP 마스커레이드를 사용해 내부 네트워크와 외부 네트워크를 분리
  - `IP 마스커레이드`: 커널에서 제공하는 NAT(Network Address Translation) 기능
- br_netfilter 를 적용함으로써 iptables 활성화

#### master_node.sh (쿠버네티스 마스터 노드로 구성하는 스크립트)

```sh
--kubeadm init --token 123456.1234567890123456 --token-ttl 0
```

- kubeadm을 통해 k8s 워커 노드를 받아들일 준비
- 토큰을 123456.1234567890123456로 지정
- ttl 을 0 으로 설정 -> 기본값인 24시간

```sh
--pod-netword-cidr=172.16.0.0./16
```

- 쿠버네티스가 자동으로 컨테이너에 부여하는 네트워크 IP

```sh
--apiserver-advertise-address=192.168.1.10
```

- 워커 노드가 접속하는 API 서버의 IP 를 192.168.1.10 로 지정해 워커 노드들이 자동으로 API 서버에 연결되게 함

```sh
kubectl apply -f \
https://raw.githubusercontent.com/sysnet4admin/IaC/master/manifests/172.16_net_calico.yaml
```

- 컨테이너 네트워크 인터페이스(CNI)인 캘리코(Calico)의 설정을 적용해 쿠버네티스 네트워크 구성

#### work_nodes.sh

```sh
kubeadm join --token 123456.1234567890123456 \
             --discovery-token-unsafe-skip-ca-verification 192.168.1.10:6443
```

- kubeadm을 이용해 쿠버네티스 마스터 노드에 접속
- 간단한 구성을 위해 `--discovery-token-unsafe-skip-ca-verification 192.168.1.10:6443` 으로 인증을 무시하고 API 서버 주소에 기본 포트 번호 6443 으로 접속

### 3.1.4 파드 배포를 중심으로 쿠버네티스 구성 요소 살펴보기

#### 쿠버네티스 구성 요소의 이름 생성 규칙

- 쿠버네티스의 구성 요소는 동시에 여러 개가 존재하는 경우 중복된 이름을 피하려고 뒤에 해시(hash) 코드가 삽입
  - 해시 코드는 무작위 문자열로 생성
- 구성 요소의 이름을 직접 지정할 수도 있지만, 구성 요소는 언제라도 문제를 발견되면 다시 생성되는 특성을 가지는 파드로 이루어져 있어서 자동으로 이름을 지정하는 것이 관리하기 쉬움
- 중간에 문자열이 더 있는 경우는 레플리카셋(ReplicaSet)을 무작위 문자열로 변형해 추가한 것

#### 관리자나 개발자가 파드를 배포할 떄

> 실행되는 순서

##### 마스터 노드

0. `kubectl`: 쿠버네티스 클러스터에 명령을 내리는 역할
   - 꼭 마스터 노드에 있을 필요는 없지만 통상적으로 API 서버와 주로 통신하므로 책에서는 API 서버가 위치한 마스터 노드에 구성
1. `API 서버`: 쿠버네티스 클러스터의 중심 역할을 하는 통로
2. `etcd`: 구성 요소들의 상태 값이 모두 저장되는 곳
   - etcd 외의 다른 구성 요소는 상태 값을 관리하지 않기 때문에 etcd 의 정보만 백업돼 있으면 긴급한 장애 상황에서도 쿠버네티스 클러스터 복구 가능
   - 분산 저장 가능한 key-value 저장소이므로, 복제해 여러 곳에 저장해 두면 하나의 etcd에서 장애가 나더라도 시스템의 가용성을 확보 가능
3. `컨트롤러 매니저`: 쿠버네티스 클러스터의 오브젝트 상태를 관리
   - 노드 컨트롤러: 워커 노드에서 통신이 되지 않는 경우, **상태 체크와 복구**
   - 레클리카셋 컨트롤러: 레플리카셋에 요청받은 파드 개수대로 파드를 생성
   - 엔드포인트 컨트롤러: 서비스와 파드를 연결
4. `스케줄러`: 노드의 상태와 자원, 레이블, 요구 조건 등을 고려해 파드를 어떤 워커 노드에 생성할 것인지 결정하고 할당

##### 워커 노드

5. `kubelet`: 파드의 구성 내용(PodSpec)을 받아서 컨테이너 런타임으로 전달하고, 파드 안의 컨테이너들이 정상적으로 작동하는지 모니터링
6. `컨테이너 런타임(CRI, Containter Runtime Interface)`: 파드를 이루는 컨테이너의 실행을 담당
7. `파드(Pod)`: 한 개 이상의 컨테이너로 단일 목적의 일을 하기 위해서 모인 단위
   - 파드는 언제라도 죽을 수 있는 존재

##### 선택 가능한 구성 요소

> 선택적으로 배포하는 것들은 순서와 상관 없음

11. `네트워크 플러그인`: 쿠버네티스 클러스터의 통신을 위해 네트워크 플러그인을 선택하고 구성해야 함.
    - 주로 CNI로 구성
12. `CoreDNS`: 클라우드 네이티브 컴퓨팅 재단에서 보증하는 프로젝트로, 빠르고 유연한 DNS 서버
    - 쿠버네티스 클러스터에서 도메인 이름을 이용해 통신하는 데 사용
    - 실무에서 쿠버네티스 클러스터를 구성하여 사용할 때는 IP 보다 도메인 네임을 편리하게 관리해 주는 CoreDNS 를 사용하는 것이 일반적

#### 사용자가 배포된 파드에 접속할 떄

1. `kube-proxy`: 쿠버네티스 클러스터는 파드가 위치한 노드에 kube-proxy를 통해 파드가 통신할 수 있는 네트워크를 설정
    - 이때 실제 통신은 br_netfilter 와 iptables 로 관리
2. `파드`: 이미 배포된 파드에 접속하고 필요한 내용을 전달 받음
    - 대부분 사용자는 파드가 어느 워커 노드에 위치하는지 신경 쓰지 않아도 됨

### 3.1.5 파드의 생명주기로 쿠버네티스 구성 요소 살펴보기

1. kubectl 을 통해 API 서버에 파드 생성을 요청
2. (업데이트가 있을 때마다 매번) API 서버에 전달된 내용이 있으면 API 서버는 etcd에 전달된 내용을 모두 기록해 클러스터의 상태 값을 최신으로 유지
3. API 서버에 파드 생성이 요청된 것을 컨트롤러 매니저가 인지하면 컨트롤러 매니저는 파드를 생성하고, 이 상태를 API 서버에 전달
    - 아직 어떤 워커 노드에 파드를 적용할지는 결정되지 않은 상태로 파드만 생성
4. API 서버에 파드가 생성됐다는 정보를 스케줄러가 인지
    - 스케줄러는 생성된 파드를 어떤 워커 노드에 적용할지 조건을 고려해 결정하고 해당 워커 노드에 파드를 띄우도록 요청
5. API 서버에 전달된 정보대로 지정한 워커 노드에 파드가 속해 있는지 스케줄러가 kubelet으로 확인
6. kubelet 에서 컨테이너 런타임으로 파드 생성을 요청
7. 파드 생성
8. 파드가 사용 가능항 상태가 됨

> - 선언적인 시스템 구조
> - 각 요소가 추구하는 상태를 선언하면 현재 상태와 맞는지 점검하고 그것에 맞추려고 노력하는 구조로 돼 있음
> - 추구하는 상태를 API 서버에 선언하면 다른 요소들이 API 서버에 와서 현재 상태와 비교하고 그에 맞게 상태를 변경하려고 함
> - API 서버와 etcd 는 거의 한몸처럼 움직이도록 설계
> - 워커 노드는 워크플로 구조, 마스터 노드는 선언적 시스템

### 3.1.6 쿠버네티스 구성 요소의 기능 검증하기

#### kubectl

- 꼭 마스터 노드에 위치할 필요 없음
  - API 서버의 접속 정보만 있다면 어느 곳에서든 쿠버네티스 클러스터에 명령을 내릴 수 있음

#### kubelet

- 파드의 생성과 상태 관리 및 복구
- kubelet 에 문제가 생기면 파드가 정상적으로 관리되지 않음

#### kube-proxy

- 파드의 통신을 담당

## 3.2 쿠버네티스 기본 사용법 배우기

### 3.2.1 파드를 생성하는 방법

- create 로 파드를 생성하려면 `kubectl create` 에 `deployment` 를 추가해서 실행해야 합니다.
- `run` 과 `create deployment` 로 파드를 생성한 것 차이
  - `run` 으로 파드를 생성하면 단일 파드 1개만 생성되고 관리
  - `create deployment` 로 파드를 생성하면 디플로이먼트(deployment) 라는 그룹 내에서 파드가 생성

### 3.2.2 오브젝트란

> 오브젝트(Object): 스펙(spec) 과 상태(status) 등의 개별 속성을 포함해 부르는 단위

#### 기본 오브젝트

- `파드(Pod)`: 쿠버네티스에서 실행되는 최소 단위
  - 독립적인 공간
  - 사용 가능한 IP
  - 1개 이상의 컨테이너
    - 여러 기능을 묶어 하나의 목적으로 사용 가능
    - 범용으로 사용할 떄는 대부분 1개의 파드에 1개의 컨테이너
- `네임스페이스(Namespaces)`: 리소스들을 구분해 관리하는 그룹
  - default: 특별히 지정하지 않으면 기본 할당
  - kube-system: 쿠버네티스 시스템에서 사용
  - metallb-system: 온프레미스에서 쿠버네티스를 사용할 경우 외부에서 쿠버네티스 클러스터 내부로 접속하게 도와주는 컨테이너들이 속함
  - ...
- `볼륨(Volume)`: 파드가 사라지더라도 저장과 보존이 가능한 디렉터리를 생성하고 사용할 수 있음
  - 파드가 생성될 때 파드에서 사용할 수 있는 디렉터리를 제공하는데 기본적으로 파드는 영속되는 개념이 아니라 제공되는 디렉터리도 임시로 사용
- `서비스(Service)`: 새로 파드가 생성될 때 부여되는 새로운 IP를 기존에 제공하던 기능과 연결
  - 파드는 클러스터 내에서 유동적이기 때문에 접속 정보가 고정일 수 없음
  - 파드 접속을 안정적으로 유지하도록 서비스를 통해 내/외부로 연결
  - 쿠버네티스 외부에서 쿠버네티스 내부로 접속할 떄 내부가 어떤 구조로 돼 있는지, 파드가 살았는지 죽었는지 신경 쓰지 않아도 이를 논리적으로 연결하는 것
  - 기존 인프라에서 로드밸런서, 게이트웨이와 비슷한 역할

#### 디플로이먼트

- 기본 오브젝트만으로도 쿠버네티스를 사용할 수 있지만, 한계가 있어서 이를 좀 더 효율적으로 작동하도록 기능들을 조합하고 추가해 구현한 것이 `디플로이먼트`
- 파드에 기반
- 레플리카셋 오브젝트를 합쳐놓은 형태

### 3.2.3 레플리카셋으로 파드 수 관리하기

> 많은 사용자를 대상으로 웹 서비스를 하려면 다수의 파드가 필요한데, 이를 하나씩 생성한다면 매우 비효율적 입니다. 그래서 쿠버네티스에서는 다수의 파드를 만드를 `레플리카셋 오브젝트`를 제공합니다.

- 특정 개수의 파드를 만들겠다고 레플리카셋에 선언하면 컨트롤러 매니저와 스케줄러가 워커 노드에 해당 개수 만큼의 파드를 만들도록 선언
- 레플리카셋은 파드 수를 보장하는 기능만 제공하기 때문에 롤링 업데이트 기능 등이 추가된 디플로이먼트를 사용해 파드 수를 관리하기를 권장
- 파드로 생성된 것은 디플로이먼트 오브젝트에 속하지 않기 때문에 scale 명령으로 replica 증가 불가능

### 3.2.4 스펙을 지정해 오브젝트 생성하기

- `create` 에서는 `replicas` 옵션을 사용할 수 없고, `scale` 은 이미 만들어진 디플로이먼트에서만 사용 가능
- 이런 설정을 적용하려면 필요한 내용을 파일로 작성해야 하는데, 이때 작성하는 파일을 `오브젝트 스펙` 이라고 함
- 쿠버네티스는 API 버전마다 포함되는 오브젝트(kind)도 달고 요구하는 내용도 다르기 때문에 처음부터 모든 내용을 숙지하기보다는 기존에 만들어진 파일을 수정하면서 이해해보고 필요한 내용을 그때마다 정리하는 것이 좋음

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo-hname
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: echo-hname
        image: sysnet4admin/echo-hname
```

```yaml
apiVersion: v1
kind: Pod
# 디플로이먼트 스펙의 template 부분과 동일
metadata:
  name: nginx-pod
spec:
  containers:
  - name: container-name
    image: nginx
```

### 3.2.5 apply로 오브젝트 생성하고 관리하기

- create 로 디플로이먼트를 생성하면 변경 사항을 바로 적용할 수 없다는 단점
  - 스펙을 변경 후 바로 적용 안됨
- `kubectl apply` 를 사용하면 파일의 변경 사항도 쉽게 적용 가능
- create 로 생성한 디플로이먼트를 apply 명령으로 수정하면 오브젝트를 처음부터 apply 로 생성한 것이 아니어서 경고 발생
- 경고가 떠도 작동에는 문제가 없지만 일관성에서 문제가 생길 수 있기 때문에 변경 사항이 발생할 가능성이 있는 오브젝트는 처음부터 apply 로 생성하는 것이 좋음
- 애드훅(ad-hoc, 일회적 사용)으로 오브젝트를 생성할 때는 create 를 사용하고, 변경이 생길 가능성이 있는 복잡한 오브젝트는 파일로 작성한 후 apply 로 적용

### 3.2.6 파드의 컨테이너 자동 복구 방법

- 쿠버네티스는 거의 모든 부분이 자동으로 복구되도록 설계
- `셀프 힐링(Self-Healing)`: 파드의 자동 복구 기술
- 제대로 작동하지 않는 컨테이너를 다시 시작하거나 교체해 파드가 정상적으로 작동하게 함

### 3.2.7 파드의 동작 보증 기능

- 파드 자체에 문제가 발생하면 파드를 자동으로 복구해서 파드가 항상 동작하도록 보장하는 기술
- 디플로이먼트에 속하지 않은 파드는 어떤 컨트롤러도 이 파드를 관리하지 않으므로 바로 삭제되고 다시 생성되조도 않음
- replicas 는 파드를 선언한 수대로 파드의 수를 항상 확인하고 부족하면 새로운 파드를 만들어내기 때문에 디플로이먼트에 속한 파드는 삭제하면 replicas 가 삭제된 파드를 확인하고 파드의 총 개수를 맞추기 위해 새로운 파드 생성
- 디플로이먼트로 생성하는 것이 파드의 동작을 보장하기 위한 조건
- 디플로이먼트에 속한 파드는 상위 디플로이먼트를 삭제해야 파드가 삭제

### 3.2.8 노드 자원 보호하기

- 노드는 쿠버네티스 스케줄러에서 파드를 할당받고 처리하는 역할
- `cordon` 기능: 문제가 발생할 가능성이 있는 노드를 스케줄 되지 않게 설정
  - 최근에 몇 차례 문제가 생긴 노드에 파드를 할당하면 문제가 생길 가능성이 높음
  - 하지만 어쩔 수 없이 해당 노드를 사용해야 할 때
  - 해당 노드는 파드가 할당되지 않게 스케줄 되지 않는 상태(SchedulingDisabled)가 됨
  - uncordon 으로 해제

### 3.2.9 노드 유지보수하기

- `drain` 기능: 지정된 노드의 파드를 전부 다른 곳으로 이동시켜 해당 노드를 유지보수할 수 있게 함
- drain 명령만 사용하면 데몬셋을 지울 수 없어서 명령을 수행할 수 없다고 하기 때문에 ignore-daemonsets 옵션과 함께 사용
- 실제로 파드를 옮기는 것이 아니라 노드에서 파드를 삭제하고 다른 곳에 다시 생성
- 해당 노드의 파드가 다른 노드로 옮겨지고 cordon 을 실행했을 때처럼 SchedulingDisabled 상태가 됨
- uncordon 을 통해 스케줄을 받을 수 있는 상태로 복귀

### 3.2.10 파드 업데이트하고 복구하기

#### 파드 업데이트하기

- `--record` : 배포한 정보의 히스토리를 기록하는 기능
  - deprecated 된 상태
- `rollout history`: record 옵션으로 기록된 히스토리 확인
- `set image` 으로 버전을 변경하면
  - replicas 의 수를 줄이고 늘려 파드를 새로 생성하며 업데이트
  - 파드들의 이름과 IP 변경
  - 시스템의 영향을 최소화하기 위해 replicas 에 속한 파드를 모두 한 번에 지우는 것이 아니라 파드를 하나씩 순차적으로 지우고 생성
- `rollout status`: 디플로이먼트 상태 확인

#### 업데이트 실패 시 파드 복구하기

- `rollout undo`: 명령 실행 취소

#### 특정 시정으로 파드 복구

- `rollout undo` + `--to-revision=<시점>`

## 3.3 쿠버네티스 연결을 담당하는 서비스

> `서비스(Service)`: 외부에서 쿠버네티스 클러스터에 접속하는 방법

### 3.3.1 가장 간단하게 연결하는 노드 포트

- 노드포트 서비스를 설정하면 모든 워커 노드의 특정 포트(노드 포트)를 열고 여기로 오는 모든 요청을 노트포트 서비스로 전달
- 노드포트 서비스는 해당 업무를 처리할 수 있는 파드로 요청을 전달
- 노드포트 서비스의 CLUSTER-IP 는 쿠버네티스 클러스터의 내부에서 사용하는 IP로 자동 지정
- 어떤 워커 노드의 IP 이든 노드포트로만 접속하면 됨

#### 부하 분산 테스트하기

- 디플로이먼트의 파드가 증가하면 부하 분산됨
- 노드포트의 오브젝트 스펙에 적힌 selector와 디플로이먼트의 이름을 확인해 동일하면 같은 파드라고 간주하도록
  - 이외 다양한 추적방법 있음

#### expose 로 노드포트 서비스 생성하기

- 노드포트 서비스는 expose 명령어로도 생성할 수 있음

### 3.3.2 사용 목적별로 연결하는 인그레스

- 노드포트 서비스는 포트를 중복 사용할 수 없어서 1개의 노드포트에 1개의 디플로이먼트만 적용됨
- 여러 개의 디플로이먼트가 있을 떄는 인그레스 사용
- `인그래스(Ingress)`
  - 고유한 주소를 제공해 사용 목적에 따라 다른 응답 제공
  - 트래픽에 대한 L4/L7 로드밸런서와 보안 인증서를 처리하는 기능
  - 인그레스 컨트롤러 필요
- `NGINX 인그레스 컨트롤러` 동작
  - 사용자는 노드마다 설정된 노드포트를 통해 노드포트로 접속
    - 이때 노드포트 서비스를 NGINX 인그레스 컨트롤러로 구성
    - 인그레스 컨트롤러는 파드와 직접 통신할 수 없어서 노드포트 또는 로드밸런서 서비스와 연동되어야 함
  - NGINX 인그레스 컨트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공
  - 클러스터 IP 서비스는 사용자를 해당 파드로 연결
- 인그레스 컨트롤러의 궁긍적인 목적은 사용자가 접속하는 경로에 따라 다른 결괏값을 제공하는 것

### 3.3.3 클라우드에서 쉽게 구성 가능한 로드밸런서

- 앞의 연결 방식은 들어오는 요청을 모두 워커 노드의 노드포트를 통해 노드포트 서비스로 이동하고 이를 다시 쿠버네티스의 파드로 보내는 구조였는데, 이 방식은 매우 비효율적
- 쿠버네티스에서는 `로드밸런서(LoadBalancer)`라는 서비스 타입을 제공해 간단한 구조로 파드를 외부에 노출하고 부하를 분산
- 로드밸런서를 사용하려면 로드밸런서를 이미 구현해 둔 서비스 업체의 도움을 받아 쿠버네티스 클러스터 외부에 구현해야 함
- 클라우드에서 제공하는 쿠버네티스를 사용하고 있다면 다음과 같이 선언하면 됨

```sh
kubectl expose deployment ex-lb --type=LoadBalancer --name=ex-svc
```

### 3.3.4 온프레미스에서 로드밸런서를 제공하는 MetalLB

- 온프레미스에서 로드밸런서를 사용하려면 내부에 로드밸런서 서비스를 받아주는 구성이 필요한데, 이를 지원하는 것이 MetalLB
- `MetalLB` 는 베어메탈(bare metal, 운영체제가 설치되지 않은 하드웨어)로 구성된 쿠버네티스에서도 로드밸런서를 사용할 수 있게 고안된 프로젝트
  - 특별한 네트워크 설정이나 구성이 있는 것이 아니라 기존의 L2 네트워크(ARP/NDP)와 L3 네트워크(BGP)로 로드밸런서를 구현
  - MetalLB 컨트롤러는 작동 방식(Protocol, 프로토콜)을 정의하고 EXTERNAL-IP 를 부여해 관리
  - MetalLB 스피커는 정해진 작동 방식(L2/ARP, L3/BGP)에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 파드의 경로를 제공
    - 이때 L2는 스피커 중에서 리더를 선출해 경로 제공을 총괄하게 함

### 3.3.5 부하에 따라 자동으로 파드 수를 조절하는 HPA

> `HPA(Horizontal Pod Autoscaler)`: 부하량에 따라 디플로이먼트의 파드 수를 유동적으로 관리하는 기능

- HPA 가 자원을 요청할 때 메트릭 서버를 통해 계측값을 전달받으므로 메트릭 서버를 설정해야 함
- edit 명령으로 배포한 디플로이먼트의 내용을 수정 가능

## 3.4 알아두면 쓸모 있는 쿠버네티스 오브젝트

### 3.4.1 데몬셋

> 디플로이먼트의 replicas가 노드 수 만큼 정해져 있는 형태 (노드 하나당 파드 한 개만 생성)

- 언제 사용?
  - 파드가 1개 이상 필요하지 않은 것
  - 노드를 관리하는 파드라면 데몬셋으로 만드는 게 가장 효율적
- 예시
  - Calico 네트워크 플러그인
  - kube-proxy
  - MetalLB 스피커

### 3.4.2 컨피그맵

> 설정(config)을 목적으로 사용하는 오브젝트

### 3.4.3 PV 와 PVC

- 파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해 공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때
  - **임시**: emptyDir
  - **로컬**: host Path, local
  - **원격**: persistentVolumeClaim, cephfs, cinder, csi, fc(fibre channel), flexVolume, flocker, glusterfs, iscsi, nfs, portworxVolume, quobyte, rbd, scaleIO, storageos, vsphereVolume
  - **특수 목적**: downwardAPI, configMap, secret, azureFile, projected
  - **클라우드**: awsElastic, azureDisk, gcePersistentDisk
- `PV`: 볼륨을 사용할 수 있게 준비하는 단계, `PVC`: 준비된 볼륨에서 일정 공간을 할당받는 것
- `PV`: 사용자가 요청할 볼륨 공간을 관리자가 만듦, `PVC`: 사용자(개발자)가 볼륨을 요청

### 3.4.4 스테이트풀셋

- 파드가 만들어지는 이름과 순서를 예측해야할 때
  - 지금까지는 파드가 replicas에 선언된 만큼 무작위로 생성
  - 주로 레디스, 주키퍼, 카산드라, 몽도DB 등의 마스터-슬레이브 구조 시스템에서 필요
- volumeClaimTemplates 기능을 사용해 PVC 자동 생성
- 각 파드가 순서대로 생성되기 때문에 고정된 이름, 볼륨, 설정
- 효율성 면에서 좋은 구조가 아니므로 요구 사항에 맞게 적절히 사용하는 것이 좋음
