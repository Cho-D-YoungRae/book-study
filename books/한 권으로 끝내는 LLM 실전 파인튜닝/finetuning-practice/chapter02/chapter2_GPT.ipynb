{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:52.225554Z",
     "start_time": "2026-02-18T21:59:46.864347Z"
    }
   },
   "source": [
    "from typing import Callable, Literal, Any\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:55.027492Z",
     "start_time": "2026-02-18T21:59:52.227168Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")",
   "id": "7b01635e2c1c4130",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:55.060081Z",
     "start_time": "2026-02-18T21:59:55.046913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = dataset\n",
    "data"
   ],
   "id": "5bd8ebe3d9eedaac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 22194\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:55.090461Z",
     "start_time": "2026-02-18T21:59:55.067401Z"
    }
   },
   "cell_type": "code",
   "source": "data[\"train\"][\"document\"][0]",
   "id": "25e3f26155c61727",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:55.101687Z",
     "start_time": "2026-02-18T21:59:55.095217Z"
    }
   },
   "cell_type": "code",
   "source": "print(sorted(list(set(data[\"train\"][\"document\"][0]))))",
   "id": "3f12d5eb4906260d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', '0', '1', '2', '3', '4', '5', 'E', 'N', 'T', 'U', 'Y', '가', '개', '것', '겠', '격', '견', '겸', '경', '고', '공', '과', '관', '국', '규', '극', '금', '급', '기', '까', '나', '난', '너', '높', '는', '늘', '니', '다', '단', '달', '담', '당', '대', '도', '되', '될', '뒷', '들', '등', '때', '또', '라', '략', '량', '러', '려', '력', '련', '로', '록', '롯', '류', '를', '리', '린', '마', '만', '말', '면', '모', '목', '무', '물', '박', '반', '받', '방', '버', '벌', '보', '복', '본', '부', '비', '산', '상', '서', '선', '성', '세', '소', '속', '쇠', '수', '스', '습', '승', '시', '실', '악', '안', '액', '앵', '야', '양', '억', '업', '에', '엔', '여', '역', '연', '열', '였', '올', '외', '용', '우', '운', '울', '원', '월', '위', '육', '율', '융', '으', '은', '을', '응', '의', '이', '인', '임', '입', '있', '자', '장', '재', '적', '전', '정', '제', '조', '주', '줄', '중', '증', '지', '진', '참', '창', '책', '척', '첨', '체', '초', '총', '최', '추', '출', '침', '커', '케', '크', '통', '투', '특', '팀', '팅', '편', '표', '하', '한', '할', '합', '해', '했', '현', '호', '홍', '화', '확', '환', '황', '회', '획', '효', '히']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:55.575309Z",
     "start_time": "2026-02-18T21:59:55.108597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ko_text = \"\".join(data[\"train\"][\"document\"])\n",
    "ko_chars = sorted(list(set(ko_text)))\n",
    "ko_vocab_size = len(ko_chars)\n",
    "print(f\"총 글자 수: {ko_vocab_size}\")"
   ],
   "id": "d9d601de19a8232d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 글자 수: 2701\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:55.587412Z",
     "start_time": "2026-02-18T21:59:55.581727Z"
    }
   },
   "cell_type": "code",
   "source": "print(ko_chars[2000:2100])",
   "id": "8ba0914b9e2c9a5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['왓', '왔', '왕', '왜', '왠', '외', '왹', '왼', '요', '욕', '욘', '욜', '욤', '욥', '용', '우', '욱', '운', '욷', '울', '움', '웁', '웃', '웅', '워', '웍', '원', '월', '웜', '웠', '웡', '웨', '웬', '웰', '웸', '웹', '웻', '위', '윅', '윈', '윌', '윔', '윕', '윗', '윙', '유', '육', '윤', '율', '윱', '윳', '융', '으', '윽', '은', '을', '음', '읍', '읏', '응', '의', '읠', '이', '익', '인', '일', '읽', '잃', '임', '입', '잇', '있', '잉', '잊', '잎', '자', '작', '잔', '잖', '잘', '잠', '잡', '잣', '잤', '장', '잦', '재', '잭', '잰', '잼', '잽', '잿', '쟁', '쟈', '쟝', '쟤', '저', '적', '전', '절']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:55.606599Z",
     "start_time": "2026-02-18T21:59:55.593796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "character_to_ids = {char:i for i, char in enumerate(ko_chars)}\n",
    "ids_to_character = {i:char for i, char in enumerate(ko_chars)}\n",
    "\n",
    "token_encode: Callable[[str], list[int]] = lambda s:[character_to_ids[c] for c in s]\n",
    "token_decode: Callable[[list[int]], str] = lambda l: \"\".join([ids_to_character[i] for i in l])\n",
    "\n",
    "print(token_encode(\"안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\"))\n",
    "print(token_decode(token_encode(\"안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\")))"
   ],
   "id": "5aa20580c200c786",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1909, 1169, 2546, 1770, 2008, 0, 2551, 1061, 0, 2064, 977, 2157, 1209, 2055, 0, 977, 1658, 2546, 949, 0, 1283, 1942, 0, 1593, 908, 2024, 2008, 2]\n",
      "안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:56.771739Z",
     "start_time": "2026-02-18T21:59:55.609105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long)\n",
    "print(tokenized_data.shape, tokenized_data.dtype)\n",
    "print(tokenized_data[:100])"
   ],
   "id": "6c6ebeee921fae04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22162967]) torch.int64\n",
      "tensor([1928, 2315,    0, 2105, 1658,  908,    0, 1987, 2555,    0, 2546, 1593,\n",
      "        1028,    0, 2015, 1485,    0,  965, 2107, 2060,    0, 1617, 2465, 1542,\n",
      "        2064,    0, 1808, 2273,    0, 2603, 1236, 1477,    0, 2037, 2555,    0,\n",
      "        2263, 1430, 2055,    0, 1028, 2019, 2062, 1028, 1441,    0, 2562, 1841,\n",
      "        1213, 1221,    2,    0, 2451, 2650,    0, 1808, 2273,    0, 2142, 1787,\n",
      "        1028, 1950, 2060,    0, 1558, 1468, 1119,    0, 2555, 1787, 1477,    0,\n",
      "        2037, 2555,    0, 1553, 1967, 1024, 2051,    0, 1015, 1541, 1477,    0,\n",
      "           7,    3, 2117,    0, 2026,    0, 2062, 1740,    0, 2603, 1236, 2546,\n",
      "         968,    0, 1558, 1468])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:56.799411Z",
     "start_time": "2026-02-18T21:59:56.779913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(tokenized_data))\n",
    "train_dataset = tokenized_data[:n]\n",
    "test_dataset = tokenized_data[n:]"
   ],
   "id": "e90c3963e881abf5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:56.820953Z",
     "start_time": "2026-02-18T21:59:56.805779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 8\n",
    "train_dataset[:block_size]"
   ],
   "id": "c97f595a9ae8d91c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1928, 2315,    0, 2105, 1658,  908,    0, 1987])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:56.837508Z",
     "start_time": "2026-02-18T21:59:56.821748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = train_dataset[:block_size]\n",
    "y = train_dataset[1:block_size+1]\n",
    "\n",
    "for time in range(block_size):\n",
    "    context = x[:time+1]\n",
    "    target = y[time]\n",
    "\n",
    "    print(f\"입력 텐서: {context}\")\n",
    "    print(f\"타깃 글자: {target}\")"
   ],
   "id": "346d41f0eb97febb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서: tensor([1928])\n",
      "타깃 글자: 2315\n",
      "입력 텐서: tensor([1928, 2315])\n",
      "타깃 글자: 0\n",
      "입력 텐서: tensor([1928, 2315,    0])\n",
      "타깃 글자: 2105\n",
      "입력 텐서: tensor([1928, 2315,    0, 2105])\n",
      "타깃 글자: 1658\n",
      "입력 텐서: tensor([1928, 2315,    0, 2105, 1658])\n",
      "타깃 글자: 908\n",
      "입력 텐서: tensor([1928, 2315,    0, 2105, 1658,  908])\n",
      "타깃 글자: 0\n",
      "입력 텐서: tensor([1928, 2315,    0, 2105, 1658,  908,    0])\n",
      "타깃 글자: 1987\n",
      "입력 텐서: tensor([1928, 2315,    0, 2105, 1658,  908,    0, 1987])\n",
      "타깃 글자: 2555\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:56.855550Z",
     "start_time": "2026-02-18T21:59:56.838318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def batch_function(mode: Literal[\"train\", \"test\"]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    return x, y\n",
    "\n",
    "example_x, example_y = batch_function(\"train\")\n",
    "print(f\"inputs: {example_x.shape}\")\n",
    "print(\"\")\n",
    "print(\"example_x의 실제 값\")\n",
    "print(example_x)\n",
    "print(\"-----------------------\")\n",
    "print(\"targets : \", example_y.shape)\n",
    "print(\"\")\n",
    "print(\"example_y의 실제 값\")\n",
    "print(example_y)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "for size in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = example_x[size, :t+1]\n",
    "        target = example_y[size, t]\n",
    "        print(f\"input: {context}, target: {target}\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"-----------------------\")"
   ],
   "id": "f30551c0cbfdccae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8])\n",
      "\n",
      "example_x의 실제 값\n",
      "tensor([[1764, 2555,    0, 1236, 2248,    0, 2017, 1976],\n",
      "        [   0, 1966, 2157,    0, 1951, 2062,    0, 2548],\n",
      "        [   0, 1304, 1485, 1586,    0, 1907, 2450,    0],\n",
      "        [   3,    2,    6,    5,    1,    0,    5,    3]])\n",
      "-----------------------\n",
      "targets :  torch.Size([4, 8])\n",
      "\n",
      "example_y의 실제 값\n",
      "tensor([[2555,    0, 1236, 2248,    0, 2017, 1976, 2546],\n",
      "        [1966, 2157,    0, 1951, 2062,    0, 2548, 2289],\n",
      "        [1304, 1485, 1586,    0, 1907, 2450,    0, 2480],\n",
      "        [   2,    6,    5,    1,    0,    5,    3,    5]])\n",
      "-----------------------\n",
      "input: tensor([1764]), target: 2555\n",
      "input: tensor([1764, 2555]), target: 0\n",
      "input: tensor([1764, 2555,    0]), target: 1236\n",
      "input: tensor([1764, 2555,    0, 1236]), target: 2248\n",
      "input: tensor([1764, 2555,    0, 1236, 2248]), target: 0\n",
      "input: tensor([1764, 2555,    0, 1236, 2248,    0]), target: 2017\n",
      "input: tensor([1764, 2555,    0, 1236, 2248,    0, 2017]), target: 1976\n",
      "input: tensor([1764, 2555,    0, 1236, 2248,    0, 2017, 1976]), target: 2546\n",
      "-----------------------\n",
      "-----------------------\n",
      "input: tensor([0]), target: 1966\n",
      "input: tensor([   0, 1966]), target: 2157\n",
      "input: tensor([   0, 1966, 2157]), target: 0\n",
      "input: tensor([   0, 1966, 2157,    0]), target: 1951\n",
      "input: tensor([   0, 1966, 2157,    0, 1951]), target: 2062\n",
      "input: tensor([   0, 1966, 2157,    0, 1951, 2062]), target: 0\n",
      "input: tensor([   0, 1966, 2157,    0, 1951, 2062,    0]), target: 2548\n",
      "input: tensor([   0, 1966, 2157,    0, 1951, 2062,    0, 2548]), target: 2289\n",
      "-----------------------\n",
      "-----------------------\n",
      "input: tensor([0]), target: 1304\n",
      "input: tensor([   0, 1304]), target: 1485\n",
      "input: tensor([   0, 1304, 1485]), target: 1586\n",
      "input: tensor([   0, 1304, 1485, 1586]), target: 0\n",
      "input: tensor([   0, 1304, 1485, 1586,    0]), target: 1907\n",
      "input: tensor([   0, 1304, 1485, 1586,    0, 1907]), target: 2450\n",
      "input: tensor([   0, 1304, 1485, 1586,    0, 1907, 2450]), target: 0\n",
      "input: tensor([   0, 1304, 1485, 1586,    0, 1907, 2450,    0]), target: 2480\n",
      "-----------------------\n",
      "-----------------------\n",
      "input: tensor([3]), target: 2\n",
      "input: tensor([3, 2]), target: 6\n",
      "input: tensor([3, 2, 6]), target: 5\n",
      "input: tensor([3, 2, 6, 5]), target: 1\n",
      "input: tensor([3, 2, 6, 5, 1]), target: 0\n",
      "input: tensor([3, 2, 6, 5, 1, 0]), target: 5\n",
      "input: tensor([3, 2, 6, 5, 1, 0, 5]), target: 3\n",
      "input: tensor([3, 2, 6, 5, 1, 0, 5, 3]), target: 5\n",
      "-----------------------\n",
      "-----------------------\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:56.921417Z",
     "start_time": "2026-02-18T21:59:56.857430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SemiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length: int):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        return logits\n",
    "\n",
    "model = SemiGPT(ko_vocab_size)\n",
    "output = model(example_x, example_y)\n",
    "print(output.shape)"
   ],
   "id": "32e0ae64bd7c7900",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2701])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:56.998809Z",
     "start_time": "2026-02-18T21:59:56.923288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SemiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length: int):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        batch, seq_length, vocab_length = logits.shape\n",
    "        logits = logits.view(batch * seq_length, vocab_length)\n",
    "        targets = targets.view(batch * seq_length)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        print(f\"logits: {logits.shape}, targets: {targets.shape}\")\n",
    "        return logits, loss\n",
    "\n",
    "model = SemiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)"
   ],
   "id": "be2e87be8ff799c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([32, 2701]), targets: torch.Size([32])\n",
      "tensor(8.5332, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T21:59:57.090264Z",
     "start_time": "2026-02-18T21:59:57.013612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SemiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length: int):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            print(f\"logits: {logits.shape}\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat([inputs, next_inputs], dim=-1)\n",
    "        return inputs\n",
    "\n",
    "model = SemiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)\n",
    "\n",
    "token_decode(model.generate(torch.zeros((1,1),\n",
    "                                        dtype=torch.long),\n",
    "                            max_new_tokens=10)[0].tolist())"
   ],
   "id": "a01e1c7f1d378eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5369, grad_fn=<NllLossBackward0>)\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 오춘令勝릐쌔탁숲동c'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T22:02:01.267189Z",
     "start_time": "2026-02-18T22:01:59.402898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 1e-2\n",
    "model = SemiGPT(ko_vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "id": "dca5e8a1f29ea78d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T22:05:46.336580Z",
     "start_time": "2026-02-18T22:05:08.535574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "for steps in tqdm(range(10000)):\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ],
   "id": "b1d9b42e92a14942",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7ac76925d9449f8981e0b30f92543bd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.436306953430176\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T22:06:26.627776Z",
     "start_time": "2026-02-18T22:06:26.602364Z"
    }
   },
   "cell_type": "code",
   "source": "print(token_decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10)[0].tolist()))",
   "id": "962453d6fada247f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      "logits: torch.Size([1, 2701])\n",
      " 콘텐츠첫鐵제시 3.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T22:07:24.154303Z",
     "start_time": "2026-02-18T22:07:24.119835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # 파이토치 2.9 이상에서는 mps 결과가 안정적입니다.\n",
    "        major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "        if (major, minor) >= (2, 9):\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = determine_device()\n",
    "print(f\"device: {device}\")"
   ],
   "id": "d07549046f2e4faf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T22:08:20.198700Z",
     "start_time": "2026-02-18T22:08:20.186499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device) # .to 를 추가\n",
    "    return x, y"
   ],
   "id": "bd345e6181b52db7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T22:11:42.784814Z",
     "start_time": "2026-02-18T22:11:42.772309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "29903d01c48fbd83",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T22:15:34.127977Z",
     "start_time": "2026-02-18T22:12:14.858751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iteration = 50000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iteration = 200\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device) # .to 를 추가\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iteration):\n",
    "    if step % eval_interval == 0 :\n",
    "        losses = compute_loss_metrics()\n",
    "        print(f'step : {step}, train loss : {losses[\"train\"]:.4f}, val loss : {losses[\"eval\"]:.4f}')\n",
    "\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(token_decode(model.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ],
   "id": "bd1ca18f525d01e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, train loss : 8.3173, val loss : 8.3162\n",
      "step : 300, train loss : 6.0513, val loss : 6.0581\n",
      "step : 600, train loss : 4.7825, val loss : 4.7623\n",
      "step : 900, train loss : 4.2123, val loss : 4.2026\n",
      "step : 1200, train loss : 3.9294, val loss : 3.9642\n",
      "step : 1500, train loss : 3.8035, val loss : 3.8076\n",
      "step : 1800, train loss : 3.7343, val loss : 3.7370\n",
      "step : 2100, train loss : 3.6446, val loss : 3.6632\n",
      "step : 2400, train loss : 3.5983, val loss : 3.6252\n",
      "step : 2700, train loss : 3.5999, val loss : 3.5938\n",
      "step : 3000, train loss : 3.5677, val loss : 3.5723\n",
      "step : 3300, train loss : 3.5587, val loss : 3.5391\n",
      "step : 3600, train loss : 3.5187, val loss : 3.5083\n",
      "step : 3900, train loss : 3.5118, val loss : 3.4898\n",
      "step : 4200, train loss : 3.4997, val loss : 3.5067\n",
      "step : 4500, train loss : 3.4748, val loss : 3.4832\n",
      "step : 4800, train loss : 3.4725, val loss : 3.4741\n",
      "step : 5100, train loss : 3.4804, val loss : 3.4709\n",
      "step : 5400, train loss : 3.4806, val loss : 3.4551\n",
      "step : 5700, train loss : 3.4695, val loss : 3.4437\n",
      "step : 6000, train loss : 3.4403, val loss : 3.4389\n",
      "step : 6300, train loss : 3.4560, val loss : 3.4470\n",
      "step : 6600, train loss : 3.4415, val loss : 3.4481\n",
      "step : 6900, train loss : 3.4347, val loss : 3.4435\n",
      "step : 7200, train loss : 3.4404, val loss : 3.4303\n",
      "step : 7500, train loss : 3.4451, val loss : 3.4233\n",
      "step : 7800, train loss : 3.4117, val loss : 3.4220\n",
      "step : 8100, train loss : 3.3976, val loss : 3.4122\n",
      "step : 8400, train loss : 3.4055, val loss : 3.4214\n",
      "step : 8700, train loss : 3.4323, val loss : 3.4212\n",
      "step : 9000, train loss : 3.4026, val loss : 3.4404\n",
      "step : 9300, train loss : 3.4082, val loss : 3.4225\n",
      "step : 9600, train loss : 3.4205, val loss : 3.4058\n",
      "step : 9900, train loss : 3.4173, val loss : 3.4152\n",
      "step : 10200, train loss : 3.4225, val loss : 3.4269\n",
      "step : 10500, train loss : 3.3994, val loss : 3.4214\n",
      "step : 10800, train loss : 3.4102, val loss : 3.4164\n",
      "step : 11100, train loss : 3.4168, val loss : 3.3971\n",
      "step : 11400, train loss : 3.4276, val loss : 3.4210\n",
      "step : 11700, train loss : 3.4171, val loss : 3.4036\n",
      "step : 12000, train loss : 3.4084, val loss : 3.4150\n",
      "step : 12300, train loss : 3.4127, val loss : 3.4121\n",
      "step : 12600, train loss : 3.4004, val loss : 3.4210\n",
      "step : 12900, train loss : 3.4040, val loss : 3.4022\n",
      "step : 13200, train loss : 3.4141, val loss : 3.3925\n",
      "step : 13500, train loss : 3.3997, val loss : 3.4069\n",
      "step : 13800, train loss : 3.4087, val loss : 3.4136\n",
      "step : 14100, train loss : 3.4036, val loss : 3.3967\n",
      "step : 14400, train loss : 3.4123, val loss : 3.3867\n",
      "step : 14700, train loss : 3.4110, val loss : 3.4110\n",
      "step : 15000, train loss : 3.4011, val loss : 3.4114\n",
      "step : 15300, train loss : 3.3895, val loss : 3.3922\n",
      "step : 15600, train loss : 3.3840, val loss : 3.4004\n",
      "step : 15900, train loss : 3.3843, val loss : 3.3936\n",
      "step : 16200, train loss : 3.4209, val loss : 3.3888\n",
      "step : 16500, train loss : 3.3845, val loss : 3.4005\n",
      "step : 16800, train loss : 3.3996, val loss : 3.3993\n",
      "step : 17100, train loss : 3.4010, val loss : 3.3891\n",
      "step : 17400, train loss : 3.4063, val loss : 3.4047\n",
      "step : 17700, train loss : 3.3917, val loss : 3.4055\n",
      "step : 18000, train loss : 3.3954, val loss : 3.4183\n",
      "step : 18300, train loss : 3.3845, val loss : 3.4008\n",
      "step : 18600, train loss : 3.3850, val loss : 3.4082\n",
      "step : 18900, train loss : 3.4078, val loss : 3.4018\n",
      "step : 19200, train loss : 3.4007, val loss : 3.4090\n",
      "step : 19500, train loss : 3.3871, val loss : 3.3996\n",
      "step : 19800, train loss : 3.4028, val loss : 3.4038\n",
      "step : 20100, train loss : 3.3992, val loss : 3.3972\n",
      "step : 20400, train loss : 3.3946, val loss : 3.4108\n",
      "step : 20700, train loss : 3.4041, val loss : 3.3865\n",
      "step : 21000, train loss : 3.3926, val loss : 3.4001\n",
      "step : 21300, train loss : 3.3844, val loss : 3.4253\n",
      "step : 21600, train loss : 3.4001, val loss : 3.4005\n",
      "step : 21900, train loss : 3.3857, val loss : 3.3976\n",
      "step : 22200, train loss : 3.3904, val loss : 3.4022\n",
      "step : 22500, train loss : 3.3833, val loss : 3.4089\n",
      "step : 22800, train loss : 3.3931, val loss : 3.4035\n",
      "step : 23100, train loss : 3.3920, val loss : 3.4036\n",
      "step : 23400, train loss : 3.3843, val loss : 3.3987\n",
      "step : 23700, train loss : 3.3839, val loss : 3.3978\n",
      "step : 24000, train loss : 3.3916, val loss : 3.4008\n",
      "step : 24300, train loss : 3.4012, val loss : 3.3997\n",
      "step : 24600, train loss : 3.3821, val loss : 3.4114\n",
      "step : 24900, train loss : 3.4001, val loss : 3.4144\n",
      "step : 25200, train loss : 3.4040, val loss : 3.4042\n",
      "step : 25500, train loss : 3.3944, val loss : 3.4185\n",
      "step : 25800, train loss : 3.3946, val loss : 3.3988\n",
      "step : 26100, train loss : 3.3958, val loss : 3.4029\n",
      "step : 26400, train loss : 3.3892, val loss : 3.3938\n",
      "step : 26700, train loss : 3.3790, val loss : 3.4086\n",
      "step : 27000, train loss : 3.3981, val loss : 3.3864\n",
      "step : 27300, train loss : 3.4012, val loss : 3.4022\n",
      "step : 27600, train loss : 3.3917, val loss : 3.4091\n",
      "step : 27900, train loss : 3.3843, val loss : 3.4127\n",
      "step : 28200, train loss : 3.3890, val loss : 3.3999\n",
      "step : 28500, train loss : 3.4034, val loss : 3.3927\n",
      "step : 28800, train loss : 3.4001, val loss : 3.4118\n",
      "step : 29100, train loss : 3.3738, val loss : 3.4196\n",
      "step : 29400, train loss : 3.3933, val loss : 3.3935\n",
      "step : 29700, train loss : 3.3987, val loss : 3.3860\n",
      "step : 30000, train loss : 3.3875, val loss : 3.4035\n",
      "step : 30300, train loss : 3.3873, val loss : 3.4126\n",
      "step : 30600, train loss : 3.3922, val loss : 3.4124\n",
      "step : 30900, train loss : 3.3850, val loss : 3.4042\n",
      "step : 31200, train loss : 3.3759, val loss : 3.4055\n",
      "step : 31500, train loss : 3.3992, val loss : 3.4142\n",
      "step : 31800, train loss : 3.3751, val loss : 3.4034\n",
      "step : 32100, train loss : 3.3913, val loss : 3.4143\n",
      "step : 32400, train loss : 3.3839, val loss : 3.4114\n",
      "step : 32700, train loss : 3.3883, val loss : 3.4010\n",
      "step : 33000, train loss : 3.3781, val loss : 3.4035\n",
      "step : 33300, train loss : 3.3901, val loss : 3.4075\n",
      "step : 33600, train loss : 3.3896, val loss : 3.3939\n",
      "step : 33900, train loss : 3.3798, val loss : 3.4072\n",
      "step : 34200, train loss : 3.4060, val loss : 3.4075\n",
      "step : 34500, train loss : 3.3767, val loss : 3.4043\n",
      "step : 34800, train loss : 3.4115, val loss : 3.3996\n",
      "step : 35100, train loss : 3.3997, val loss : 3.3980\n",
      "step : 35400, train loss : 3.3732, val loss : 3.3908\n",
      "step : 35700, train loss : 3.3794, val loss : 3.4088\n",
      "step : 36000, train loss : 3.3954, val loss : 3.4014\n",
      "step : 36300, train loss : 3.3997, val loss : 3.3982\n",
      "step : 36600, train loss : 3.3999, val loss : 3.3976\n",
      "step : 36900, train loss : 3.3764, val loss : 3.3920\n",
      "step : 37200, train loss : 3.4084, val loss : 3.3998\n",
      "step : 37500, train loss : 3.3984, val loss : 3.3930\n",
      "step : 37800, train loss : 3.3826, val loss : 3.4117\n",
      "step : 38100, train loss : 3.3962, val loss : 3.4061\n",
      "step : 38400, train loss : 3.3989, val loss : 3.4030\n",
      "step : 38700, train loss : 3.3893, val loss : 3.4076\n",
      "step : 39000, train loss : 3.3991, val loss : 3.4200\n",
      "step : 39300, train loss : 3.4015, val loss : 3.3889\n",
      "step : 39600, train loss : 3.4022, val loss : 3.3984\n",
      "step : 39900, train loss : 3.3839, val loss : 3.3905\n",
      "step : 40200, train loss : 3.3921, val loss : 3.4065\n",
      "step : 40500, train loss : 3.3832, val loss : 3.3927\n",
      "step : 40800, train loss : 3.3988, val loss : 3.4025\n",
      "step : 41100, train loss : 3.3861, val loss : 3.3950\n",
      "step : 41400, train loss : 3.3798, val loss : 3.3976\n",
      "step : 41700, train loss : 3.4070, val loss : 3.3878\n",
      "step : 42000, train loss : 3.3912, val loss : 3.3856\n",
      "step : 42300, train loss : 3.4071, val loss : 3.4042\n",
      "step : 42600, train loss : 3.3879, val loss : 3.3976\n",
      "step : 42900, train loss : 3.3898, val loss : 3.4123\n",
      "step : 43200, train loss : 3.3840, val loss : 3.3943\n",
      "step : 43500, train loss : 3.3855, val loss : 3.3981\n",
      "step : 43800, train loss : 3.3952, val loss : 3.4088\n",
      "step : 44100, train loss : 3.3888, val loss : 3.3979\n",
      "step : 44400, train loss : 3.3880, val loss : 3.3868\n",
      "step : 44700, train loss : 3.4199, val loss : 3.4044\n",
      "step : 45000, train loss : 3.3970, val loss : 3.4103\n",
      "step : 45300, train loss : 3.3877, val loss : 3.4010\n",
      "step : 45600, train loss : 3.3903, val loss : 3.4022\n",
      "step : 45900, train loss : 3.3842, val loss : 3.3792\n",
      "step : 46200, train loss : 3.3867, val loss : 3.4110\n",
      "step : 46500, train loss : 3.3805, val loss : 3.4023\n",
      "step : 46800, train loss : 3.3802, val loss : 3.3993\n",
      "step : 47100, train loss : 3.3927, val loss : 3.4101\n",
      "step : 47400, train loss : 3.3951, val loss : 3.3846\n",
      "step : 47700, train loss : 3.3868, val loss : 3.3941\n",
      "step : 48000, train loss : 3.4008, val loss : 3.4085\n",
      "step : 48300, train loss : 3.3802, val loss : 3.4101\n",
      "step : 48600, train loss : 3.3868, val loss : 3.3915\n",
      "step : 48900, train loss : 3.3946, val loss : 3.3973\n",
      "step : 49200, train loss : 3.3815, val loss : 3.4024\n",
      "step : 49500, train loss : 3.3786, val loss : 3.3899\n",
      "step : 49800, train loss : 3.3944, val loss : 3.4001\n",
      " 전 유동이 항을 제1%에도 등으로 출시장은석한 그리의 이를 주된 냈다.8천2일 커맨드 수출시그러시면 수술로 건강 연구위는 6㎞力X 양국민과했는 당하고 작년 120일 유동에 투기에 \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7cd7a6ab5da0f8fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
